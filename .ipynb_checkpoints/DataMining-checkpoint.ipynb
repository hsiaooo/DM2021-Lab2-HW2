{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infectious-buddy",
   "metadata": {},
   "source": [
    "# Week 14: Senquence Classification with BERT\n",
    "\n",
    "The assignment this week is to do the senquence classification. This may sound like what we had done in the previous assignment, but we are using BERT as our classifier this week, rather than Machine Learning.\n",
    "\n",
    "The objective is to judge the CEFR level of a sentence.  \n",
    "[CEFR](https://www.cambridgeenglish.org/exams-and-tests/cefr/) is a standard for describing language ability of a person. It consists of 6 levels, A1, A2, B1, B2, C1, and C2, going from easier to harder.  \n",
    "A dataset that contains sentences with the corresponding CEFR level is provided, and you have to use BERT and train a sentence classifier with this dataset.  \n",
    "The dataset is collected and processed from a research by Alison Chi, ÊùéÊõ∏Âçâ, ÊùéÂÜ†Èúñ and Prof. Chang. Thank you all for allowing us to use it in the lecture.\n",
    "\n",
    "As to the implementatin, we will introduce you the [ü§ó transformers](https://huggingface.co/) library, which is mantained by huggingface company, as the training framework this week. [Pytorch](https://pytorch.org/) is used as the deep learning backend in this tutorial, but with the transformers library, all codes can be easily changed to tensorflow if you prefer so.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-speaker",
   "metadata": {},
   "source": [
    "## Prepare your environment\n",
    "\n",
    "Again, we highly recommend you to install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  \n",
    "\n",
    "If you haven't used it before and don't know which to use, I would suggest you start with [mamba](https://github.com/mamba-org/mamba#installation) or [mambaforge](https://github.com/conda-forge/miniforge#mambaforge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9c26f",
   "metadata": {},
   "source": [
    "### Install CUDA\n",
    "\n",
    "Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n",
    "To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  \n",
    "\n",
    "If you already had CUDA installed on your machine, then great! You're done here.  \n",
    "If you don't, you can refer to [Appendix 1](#Appendix-1-Install-CUDA) to see how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c0036",
   "metadata": {},
   "source": [
    "### Install python packages\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "1. `numpy`: for matrix operation\n",
    "2. `scikit-learn`: for label encoding\n",
    "3. `datasets`: for data preparation\n",
    "4. `transformers`: for model loading and finetuing\n",
    "5. (choose one) `tensorflow` / `pytorch`: the backend DL framework\n",
    "   - Note that the tf/pt version must support the CUDA version you've installed if you want to use GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18cc24",
   "metadata": {},
   "source": [
    "### Select GPU(s) for your backend\n",
    "\n",
    "Skip this section if you have no intension of using GPU with tensorflow/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c2f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa1f9c",
   "metadata": {},
   "source": [
    "#### >> Check Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254c8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if any GPU is used\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69ce74",
   "metadata": {},
   "source": [
    "#### >> Check Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e144684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Check if your GPU(s) is(are) listed below \n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-radio",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Before starting the training, of course we need to load and process our dataset - but wait a sec. Let's decide which model we want to use first.  \n",
    "\n",
    "In case you are not familiar with it, [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is a language model proposed by Google AI in 2018, and it's one of the most popular models used in NLP area.  \n",
    "However, we will not directly use BERT in this tutorial, because it's large and needs plenty of time to train. Instead, we are using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5) this week.  \n",
    "\n",
    "DistilBERT is a distilled (Ëí∏È§æ) version of BERT that is much more light-weighted than original model while reserving 95% of its original accuracy, which makes it perfect for our task today.  \n",
    "\n",
    "Further Reading:\n",
    " - [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) by Samia, 2019.\n",
    " - [ÈÄ≤ÊìäÁöÑ BERTÔºöNLP ÁïåÁöÑÂ∑®‰∫∫‰πãÂäõËàáÈÅ∑ÁßªÂ≠∏Áøí](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html) by ÊùéÂ≠ü, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "manufactured-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "#MODEL_NAME = 'distilbert-base-uncased'\n",
    "MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-woman",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Similar to `transformers` library, `datasets` is also a package provided by huggingface. It contains many public datasets online and can help us with the data processing.  \n",
    "We can use `load_dataset` function to read the input `.csv` file.\n",
    "\n",
    "Reference:\n",
    " - [Official datasets document](https://huggingface.co/docs/datasets)\n",
    " - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "personalized-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minute-tension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-790bd679a5cc5029\n",
      "Reusing dataset csv (/home/neaf-3090/.cache/huggingface/datasets/csv/default-790bd679a5cc5029/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = 'train_df_noemoji.csv')\n",
    "#dataset = load_dataset('csv', data_files = os.path.join('data', 'train_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9986f-15ef-436f-9d93-968475ebeddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ccd2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'emotion'],\n",
      "    num_rows: 1455563\n",
      "})\n",
      "{'text': 'brianklaas As we see Trump is dangerous to freepress around the world What a LH LH TrumpLegacy  CNN', 'emotion': 'sadness'}\n",
      "['People who post add me on Snapchat must be dehydrated Cuz man thats LH', 'brianklaas As we see Trump is dangerous to freepress around the world What a LH LH TrumpLegacy  CNN', 'Now ISSA is stalking Tasha  LH', 'RISKshow TheKevinAllison Thx for the BEST TIME tonight What stories Heartbreakingly LH authentic LaughOutLoud good', 'Still waiting on those supplies Liscus LH', 'Love knows no gender  LH', 'DStvNgCare DStvNg More highlights are being shown than actual sports Who watches triathlon highlights anyway LH LeagueCup', 'The SSM debate LH a manufactured fantasy used to distract the ignorant masses from their mundane lives V gender diversity a m', 'I love suffering  I love when valium does nothing to help  I love when my doctors say that theyve done all they can  LH', 'Can someone tell my why my feeds scroll back to the same 30 tweets that I saw 1 min ago Pissed']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data structure\n",
    "print(dataset['train'])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train']['text'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc16be",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Same as before, texts should be tokenized, embedded, and padded before put into the model.  \n",
    "But don't worry, with the libraries from huggingface, the procedure is much easier now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302e662",
   "metadata": {},
   "source": [
    "#### Sentence processing\n",
    "\n",
    "Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n",
    "\n",
    "With huggingface, loading different tokenizer is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n",
    "\n",
    "Reference:\n",
    " - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831e6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # For tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323c2f3",
   "metadata": {},
   "source": [
    "#### Play with BERTTokenizer\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with BERTTokenizer.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b908a",
   "metadata": {},
   "source": [
    "Let's play with this tokenizer a little bit before we go on.\n",
    "\n",
    "Using this tokenizer is pretty easy: just call this object, and it processes the sentences for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f39baab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2061, 1011, 2170, 1000, 3819, 3944, 1000, 2001, 2061, 15640, 1010, 2004, 2092, 2004, 12532, 4648, 4726, 2149, 2013, 2746, 2000, 2115, 4418, 3004, 2153, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\"\n",
    "\n",
    "embeddings = tokenizer(example)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040ddd8",
   "metadata": {},
   "source": [
    "As you can see, the sentence has already been tokenized and embedded. A default attention mask is returned as well.  \n",
    "\n",
    "To get the token back is easy as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033ecb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] this so - called \" perfect evening \" was so disappointing , as well as disco ##ura ##ging us from coming to your circle theatre again . [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens = tokenizer.batch_decode(embeddings['input_ids'])\n",
    "print(' '.join(decoded_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f093abd",
   "metadata": {},
   "source": [
    "You may notice that there're some weird stuffs appearing in your task, like `[CLS]` or `[SEP]`. The word *discouraging* is even split into `disco` `##ura` and `##ging` .  \n",
    "`[CLS]`, `[SEP]`, `[UKN]` and `[MASK]` are four symbols introduced by BERT, which stand for \"classification\", \"seperator\", \"unknown\" and \"mask\" respectively.  \n",
    "As to `##` thing, it's called a *wordpiece*, which is a concept [also brought out by Google](https://arxiv.org/abs/1609.08144). The key idea is to split words into common sub-word units, so the number of rare words can significantly decrease.\n",
    "\n",
    "Besides simply tokenizing a sentence, there are also many parameters you can set. You can play with it a bit, changing the parameters and observe the difference.\n",
    "\n",
    "Document:\n",
    " - [transformers.Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1118f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2061,  1011,  2170,  1000,  3819,  3944,  1000,  2001,\n",
       "          2061, 15640,  1010,  2004,  2092,  2004, 12532,  4648,  4726,  2149,\n",
       "          2013,  2746,  2000,  2115,  4418,  3004,  2153,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE: directly transform into embedding tensor\n",
    "embeddings = tokenizer(example,\n",
    "                       # padding='longest',         # padding strategy\n",
    "                       # max_length=10,             # how long to pad sentences\n",
    "                       is_split_into_words=False,\n",
    "                       truncation=True,\n",
    "                       return_tensors='pt',         # 'tf' for tensofrlow, 'pt' for pytorch, 'np' for numpy\n",
    "                       # return_length=True         # whether to return length\n",
    "                       # Any other parameters you want to try\n",
    "                      )\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e667000",
   "metadata": {},
   "source": [
    "#### Label processing\n",
    "\n",
    "Before we start to process sentences in the whole dataset, don't forget we need to process labels as well.\n",
    "\n",
    "In the following section, I will introduce you the OneHotEncoder provided by scikit-learn.\n",
    "\n",
    "Documents:\n",
    " - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "135b2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# First, declare a new encoder\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "# Then, let the encoder learns all features in the given dataset\n",
    "# Keep in mind that all `fit` functions in sklearn only make the encoder learn from the data, not transforming the data yet.\n",
    "encoder = encoder.fit(np.reshape(dataset['train']['emotion'], (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b71cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "LABEL_COUNT = len(encoder.categories_[0])\n",
    "print(LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f7e29",
   "metadata": {},
   "source": [
    "#### Play with OneHotEncoder\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with sklearn.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b30ebc",
   "metadata": {},
   "source": [
    "One thing you should always keep in mind is: features learned by OneHotEncoder are always treated as arrays, because it allows multi-field features. (See its [document](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder) for an example)  \n",
    "That's why you have to reshape the level into (-1, 1), i.e. from `['A1', 'B1', 'C1', ...]` to `[['A1'], ['B1'], ['C1'], ...]` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4a89d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness',\n",
      "       'surprise', 'trust'], dtype='<U12')]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what features has the encoder captured\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63ee709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# use `encoder.transform` to get the one-hot code of a label\n",
    "print(encoder.transform([['anger'], ['fear']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9501dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['disgust']]\n"
     ]
    }
   ],
   "source": [
    "# To decode, use `encoder.inverse_transform` instead\n",
    "print(encoder.inverse_transform([[0, 0, 1, 0, 0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72153561",
   "metadata": {},
   "source": [
    "#### [ TODO ] Process the data\n",
    "\n",
    "With the tokenizor and encoder prepared, we can write a function to process our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5669f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataslice):\n",
    "    \"\"\" Input: a batch of your dataset\n",
    "        Example: { 'text': [['sentence1'], ['setence2'], ...],\n",
    "                   'label': ['label1', 'label2', ...] }\n",
    "    \"\"\"\n",
    "    dataset = {}\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    emotion_list = []\n",
    "    text_list = []\n",
    "    \n",
    "    for text, emotion in zip(dataslice['text'], dataslice['emotion']):\n",
    "        text = text.replace(\"LH\", \"\").replace(\"  \", \" \").lower()\n",
    "        embeddings = tokenizer(text,\n",
    "                       padding=True,         # padding strategy\n",
    "                       # max_length=10,             # how long to pad sentences\n",
    "                       is_split_into_words=False,\n",
    "                       truncation=True,\n",
    "                       return_tensors='pt',         # 'tf' for tensofrlow, 'pt' for pytorch, 'np' for numpy\n",
    "                       # return_length=True         # whether to return length\n",
    "                       # Any other parameters you want to try\n",
    "                      )\n",
    "        input_ids_list.append(embeddings['input_ids'][0])\n",
    "        token_type_ids_list.append(embeddings['token_type_ids'][0])\n",
    "        attention_mask_list.append(embeddings['attention_mask'][0])\n",
    "        #label_list.append(encoder.transform(np.reshape(label, (-1, 1))))\n",
    "        emotion_list.append([emotion])\n",
    "        text_list.append(text)\n",
    "        \n",
    "        \n",
    "        \n",
    "    dataset['input_ids'] = input_ids_list\n",
    "    dataset['token_type_ids'] = token_type_ids_list\n",
    "    dataset['attention_mask'] = attention_mask_list\n",
    "    dataset['label'] = encoder.transform(emotion_list)\n",
    "    dataset['emotion'] = emotion_list\n",
    "    dataset['text'] = text_list\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "    \n",
    "    # [ TODO ]\n",
    "    ...\n",
    "\n",
    "    \"\"\" Output: a batch of processed dataset\n",
    "        Example: { 'input_ids': ...,\n",
    "                   'attention_masks': ...,\n",
    "                   'label': ... }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc612333",
   "metadata": {},
   "source": [
    "Now, map the function to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa298f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89ed2f474c94ecc86c41012213decf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1456 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data = dataset.map(preprocess,    # your processing function\n",
    "                             batched = True # Process in batches so it can be faster\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74953918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'emotion', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
      "        num_rows: 1455563\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'emotion': ['anticipation'],\n",
       " 'input_ids': [101,\n",
       "  2111,\n",
       "  2040,\n",
       "  2695,\n",
       "  5587,\n",
       "  2033,\n",
       "  2006,\n",
       "  10245,\n",
       "  7507,\n",
       "  2102,\n",
       "  2442,\n",
       "  2022,\n",
       "  2139,\n",
       "  10536,\n",
       "  7265,\n",
       "  3064,\n",
       "  12731,\n",
       "  2480,\n",
       "  2158,\n",
       "  2008,\n",
       "  2015,\n",
       "  102],\n",
       " 'label': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'text': 'people who post add me on snapchat must be dehydrated cuz man thats ',\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at processed dataset\n",
    "print(processed_data)\n",
    "processed_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670f7c6",
   "metadata": {},
   "source": [
    "### DataCollator\n",
    "\n",
    "You may notice that we didn't pad the sentences in the preprocessing function, because we are going to do it during the training time.  \n",
    "\n",
    "To do the training-time processing, we can use the DataCollator Class provided by `transformers`. What's even better is, transformers already provides a class that handles padding for us!\n",
    "\n",
    " - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1add1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# declare a collator to do padding during traning.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be824b94",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57154197",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "We can load the pretrained model from `transformers`.  \n",
    "Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, all works can be done in two lines of codes: \n",
    "\n",
    "1. Load `AutoModelForSequenceClassification` Class.\n",
    "2. Load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a09afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Change to TFAutoModelForSequenceClassification if you're using tensoflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                           num_labels = LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d085bc",
   "metadata": {},
   "source": [
    "#### [ TODO ] Split train/val data\n",
    "\n",
    "The `Dataset` class we prepared before already has the `train_test_split` method. You can use it to split your dataset.\n",
    "\n",
    "Document:\n",
    " - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9352c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/neaf-3090/.cache/huggingface/datasets/csv/default-27ef3c0bbd90db4a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-5812b656b1494ff3.arrow and /home/neaf-3090/.cache/huggingface/datasets/csv/default-27ef3c0bbd90db4a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-d4937ddb40026fe9.arrow\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] Choose the validation data size                                v here\n",
    "train_val_dataset = processed_data['train'].train_test_split(test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545bc915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'emotion', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
      "        num_rows: 1310006\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'emotion', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
      "        num_rows: 145557\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at split data\n",
    "print(train_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0667bc9",
   "metadata": {},
   "source": [
    "#### [ TODO ] Setup training parameters\n",
    "\n",
    "We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n",
    "\n",
    "Document:\n",
    "- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189a7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to TFTrainingArguments, TFTrainer if you're using tensoflow\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf81339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ TODO ] Set and tune your training properties\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 8\n",
    "#WARMUP = 0.1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'model',\n",
    "    #overwrite_output_dir = True,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    #per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    load_best_model_at_end=True,\n",
    "    #warmup_ratio = WARMUP,\n",
    "    # You can also set other parameters here\n",
    ")\n",
    "\n",
    "# Now give all information to a trainer.\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = processed_data['train'],\n",
    "    #train_dataset = train_val_dataset[\"train\"],\n",
    "    #eval_dataset = train_val_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    \n",
    "    # You can also set other parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd2b6d",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is pretty easy. Simply ask the trainer to train the model for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bae54732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90976' max='90976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90976/90976 3:44:24, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>0.077200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90976, training_loss=0.1418504040718414, metrics={'train_runtime': 13465.0756, 'train_samples_per_second': 864.793, 'train_steps_per_second': 6.756, 'total_flos': 2.9288245545525286e+17, 'train_loss': 0.1418504040718414, 'epoch': 8.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63e5b3",
   "metadata": {},
   "source": [
    "You can see that Trainer saves some ckeckpoints, so you can load your model from those checkpoints if you want to fallback to a specific version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1501549",
   "metadata": {},
   "source": [
    "### Save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d400687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join('model', 'finetuned'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af492d0",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We've known how to train a model now, but how to really use it for predicting results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587893d",
   "metadata": {},
   "source": [
    "### Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71dc129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, change to TFxxxxxx if you are using tensorflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', 'finetuned'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583078f",
   "metadata": {},
   "source": [
    "### Get the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d987e",
   "metadata": {},
   "source": [
    "Given six example sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5623b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    # A2\n",
    "    \"Remember to write me a letter.\",\n",
    "    # B2\n",
    "    \"Strawberries and cream - a perfect combination.\",\n",
    "    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\n",
    "    # C1\n",
    "    \"Some may altogether give up their studies, which I think is a disastrous move.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10a6dd",
   "metadata": {},
   "source": [
    "...all you need to do is to transform them to embeddings, and then you can get predictions by calling your finetuned model.  \n",
    "\n",
    "Note that, since you don't have a DataCollator to pad the sentence and do the matrix transformation for you, you have to pad and transform the matrice on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2276924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6909,  0.7161, -3.7188, -4.3224, -3.6388, -4.1544],\n",
       "        [-5.4560, -5.4394, -3.9893,  2.4557, -3.0997, -2.8560],\n",
       "        [-5.0532, -5.2660, -4.2828,  2.7641, -2.9263, -3.7443],\n",
       "        [-4.8139, -4.8385, -4.6691, -2.5907,  2.7176, -4.7137]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the sentences into embeddings\n",
    "input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\") # change return_tensors if youre using tensorflow\n",
    "# Get the output\n",
    "logits = mymodel(**input).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1e99d",
   "metadata": {},
   "source": [
    "Logits aren't very readable for us. Let's use softmax activation to transform them into more probability-like numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2ea86b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9076e-01, 7.7898e-01, 9.2353e-03, 5.0503e-03, 1.0005e-02, 5.9746e-03],\n",
       "        [3.6239e-04, 3.6846e-04, 1.5709e-03, 9.8900e-01, 3.8237e-03, 4.8791e-03],\n",
       "        [4.0011e-04, 3.2343e-04, 8.6450e-04, 9.9357e-01, 3.3564e-03, 1.4813e-03],\n",
       "        [5.3209e-04, 5.1918e-04, 6.1502e-04, 4.9150e-03, 9.9283e-01, 5.8820e-04]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or `from tensorflow import nn` and `nn.softmax`\n",
    "from torch import nn\n",
    "\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a08934",
   "metadata": {},
   "source": [
    "#### [ TODO ] transform logits back to labels\n",
    "\n",
    "Now you've got the output. Write a function to map it back into labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6fba1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ TODO ] try to process the result\n",
    "def transform_labels(logits):\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    if isinstance(logits, list):\n",
    "        logits = np.array(logits)\n",
    "    a = np.argmax(logits, axis=1)\n",
    "    b = np.zeros((len(a), logits.shape[1]))\n",
    "    b[np.arange(len(a)), a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9078321c-cb77-4f15-a7e4-ba1ce7833bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['C1']]\n"
     ]
    }
   ],
   "source": [
    "predicts = transform_labels(predicts)\n",
    "print(encoder.inverse_transform(predicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec453e",
   "metadata": {},
   "source": [
    "## [ TODO ] Evaluation\n",
    "\n",
    "It's your turn!  \n",
    "Load the testing data and calculate your accuracy.\n",
    "\n",
    "We want you to calculate two kinds of accuracy, exact accuracy and fuzzy accuracy, which will be explained in the following section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9c92380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-473e3bdc78202b1e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/neaf-3090/.cache/huggingface/datasets/csv/default-473e3bdc78202b1e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/neaf-3090/.cache/huggingface/datasets/csv/default-473e3bdc78202b1e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['text', 'emotion'],\n",
      "    num_rows: 411972\n",
      "})\n",
      "{'text': 'Trust is not the same as faith A friend is someone you trust Putting faith in anyone is a mistake  Christopher Hitchens LH LH', 'emotion': None}\n",
      "['Confident of your obedience I write to you knowing that you will do even more than I ask Philemon 121 34 bibleverse LH LH', 'Trust is not the same as faith A friend is someone you trust Putting faith in anyone is a mistake  Christopher Hitchens LH LH', 'When do you have enough  When are you satisfied  Is you goal really all about money   materialism money possessions LH', 'God woke you up now chase the day GodsPlan GodsWork LH', 'In these tough times who do YOU turn to as your symbol of hope LH']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c38e6f5fd44a8c8b82ccf5d504444e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [None] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f189e48089f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m processed_evaldata = eval_dataset.map(preprocess,    # your processing function\n\u001b[0;32m---> 13\u001b[0;31m                              \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# Process in batches so it can be faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                             )\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 )\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             }\n\u001b[1;32m    491\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 )\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             }\n\u001b[1;32m    491\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   1680\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m             )\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         }\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m   2018\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m                                 \u001b[0mcheck_same_num_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2020\u001b[0;31m                                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2021\u001b[0m                             )\n\u001b[1;32m   2022\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mNumExamplesMismatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 \u001b[0meffective_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m             processed_inputs = (\n\u001b[0;32m-> 1906\u001b[0;31m                 \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m             )\n\u001b[1;32m   1908\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-22437ffb568d>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(dataslice)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_type_ids_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown,\n\u001b[0;32m--> 472\u001b[0;31m                                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    135\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories [None] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "# [ TODO ] \n",
    "# load test data\n",
    "# preprocess\n",
    "# get predictions\n",
    "# transform predictions back into labels\n",
    "\n",
    "#eval_dataset = load_dataset('csv', data_files = os.path.join('data', 'test_df.csv'))\n",
    "eval_dataset = load_dataset('csv', data_files = 'test_df_noemoji.csv')\n",
    "print(eval_dataset['train'])\n",
    "print(eval_dataset['train'][1])\n",
    "print(eval_dataset['train']['text'][:5])\n",
    "processed_evaldata = eval_dataset.map(preprocess,    # your processing function\n",
    "                             batched = True # Process in batches so it can be faster\n",
    "                            )\n",
    "print(\"-----------\")\n",
    "print(processed_evaldata)\n",
    "print(processed_evaldata['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1657ac91-7d7a-41de-acfc-343e44afc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "mymodel = mymodel.cpu()\n",
    "classifier = pipeline('sentiment-analysis', model=mymodel, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f434d70-b44d-4e5d-9b74-a7ff1e85d52a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_evaldata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-042379351ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_evaldata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpredict_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_evaldata' is not defined"
     ]
    }
   ],
   "source": [
    "predict_list = []\n",
    "label_list = []\n",
    "print_num = 0\n",
    "for i in processed_evaldata['train']:\n",
    "    result = classifier(i['text'])[0]\n",
    "    predict_list.append(int(result['label'][-1]))\n",
    "    label_list.append(int(np.argmax(i['label'])))\n",
    "    #if print_num < 10:\n",
    "        #print(result['label'])\n",
    "        #print_num += 1\n",
    "#print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffd72a5e-1c23-47fd-a268-06a7b4e42564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(3, 3)\n",
      "(3, 3)\n",
      "(3, 5)\n",
      "(4, 4)\n",
      "(1, 1)\n",
      "(2, 2)\n",
      "(3, 3)\n",
      "(2, 1)\n",
      "(5, 4)\n",
      "(3, 3)\n",
      "(4, 4)\n",
      "(4, 4)\n",
      "(4, 4)\n",
      "(3, 2)\n",
      "(3, 2)\n",
      "(4, 4)\n",
      "(3, 5)\n",
      "(1, 4)\n",
      "(4, 4)\n",
      "(4, 3)\n",
      "(3, 5)\n",
      "(3, 3)\n",
      "(4, 4)\n",
      "(0, 1)\n",
      "(2, 2)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "(1, 1)\n",
      "(5, 5)\n",
      "(3, 2)\n",
      "(1, 1)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "(3, 3)\n",
      "(4, 5)\n",
      "(3, 3)\n",
      "(3, 3)\n",
      "(2, 3)\n",
      "(2, 2)\n",
      "(0, 1)\n",
      "(3, 5)\n",
      "(3, 5)\n",
      "(2, 1)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(3, 3)\n",
      "(2, 3)\n",
      "(3, 2)\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print_num = 0\n",
    "for p, l in zip(predict_list, label_list):\n",
    "    if print_num < 50:\n",
    "        print((p, l))\n",
    "        print_num += 1\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cef45a",
   "metadata": {},
   "source": [
    "### Six Level Accuracy\n",
    "\n",
    "Exact accuracy is what you've been familiar with:\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "                    ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{3}{6} = 0.5$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6545d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5569565217391305\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for p, l in zip(predict_list, label_list):\n",
    "    if p == l:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa2778",
   "metadata": {},
   "source": [
    "### [ TODO ] Three Level Accuracy\n",
    "\n",
    "Three Level Accuracy is used when you only want the general of right or wrong.\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "              ^     ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{4}{6} = 0.667$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75bc6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7421739130434782\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "label_a = [0, 1]\n",
    "label_b = [2, 3]\n",
    "label_c = [4, 5]\n",
    "for p, l in zip(predict_list, label_list):\n",
    "    if p in label_a and l in label_a:\n",
    "        correct += 1\n",
    "    elif p in label_b and l in label_b:\n",
    "        correct += 1\n",
    "    elif p in label_c and l in label_c:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae91c91",
   "metadata": {},
   "source": [
    "### [ TODO ] Fuzzy accuracy\n",
    "\n",
    "However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n",
    "\n",
    "For example, if the label is actually 'B1', but the model predicts 'B2', we still consider the prediction good enough, and this also applys when the model predicts 'A2'.\n",
    "\n",
    "Hence, the fuzzy accuracy is\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   0 1 2 3 4 5\n",
    "Ground truth: 0 1 1 3 3 3\n",
    "              ^ ^ ^ ^ ^\n",
    "```\n",
    "\n",
    "The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n",
    "\n",
    "As the requirement, <u>your accuracy should be higher than $0.8$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5b853c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8291304347826087\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] calculate accuracy\n",
    "for p, l in zip(predict_list, label_list):\n",
    "    if np.abs(p-l) <= 1:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404b1bf",
   "metadata": {},
   "source": [
    "## TA's note\n",
    "\n",
    "Congratuation! You've finished the assignment this week.  \n",
    "Don't forget to <b>[make an appoiment with TA](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=134737606) to demo/explain your implementation <u>before <font color=\"red\">12/23 15:30</font></u></b> .  \n",
    "Also make sure you submit your `{student_id}.ipynb` to [eeclass](https://eeclass.nthu.edu.tw/course/homework/6053).\n",
    "\n",
    "This is the last assignment of this class. A TA will still be at the online classroom and answer your question during the class time in the following weeks, and you can have make-up demos at that time.  \n",
    "Prof. Chang's office hours are in Tues. to Thurs. evenings. You can come to Delta 712 to consult him at that time, but make sure you follow the appointment rules written on the bulletin or [the appointment sheet](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7eb0",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "\n",
    "<a name=\"Appendix-1-Install-CUDA\"></a>\n",
    "\n",
    "### Appendix 1 - Install CUDA\n",
    "\n",
    "1. Check your GPU vs. CUDA compatibility:\n",
    "   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n",
    "2. Check library vs. CUDA compatibility: \n",
    "   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n",
    "   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n",
    "3. Note the highest CUDA version that fits your system.\n",
    "\n",
    "#### >> for conda/mamba users\n",
    "\n",
    "You can directly install CUDA library with the selected CUDA version.\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n",
    "\n",
    "#### >> for non-conda users\n",
    "\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0dd1d",
   "metadata": {},
   "source": [
    "<a name=\"Appendix-2-TAs-Environmental-setup\"></a>\n",
    "\n",
    "### Appendix 2 - TA's Environmental Setup\n",
    "\n",
    "The following is my setup for this notebook. You can refer to it if you encounter some environmental issues.  \n",
    "\n",
    "System: Ubuntu 18.04.6, x64, With GPU support. All packages are installed in new conda environment with channels default to conda-forge.\n",
    "\n",
    "1. Python 3.8.12\n",
    "2. numpy=1.21.2\n",
    "3. cudatoolkit=11.1.74\n",
    "4. pytorch=1.8.2\n",
    "5. datasets=1.16.1\n",
    "6. transformers=4.12.5\n",
    "7. scikit-learn=1.0.1\n",
    "\n",
    "Notes:\n",
    "\n",
    " - conda create -n week14 python=3.8 & conda activate week14\n",
    " - conda config --add channels conda-forge\n",
    " - conda config --set channel_priority strict\n",
    " - conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\n",
    " - conda install transformers\n",
    " - conda install datasets scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0f256",
   "metadata": {},
   "source": [
    "### Appendix 3 - Further Readings\n",
    "\n",
    "1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n",
    "2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n",
    "3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n",
    "4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
    "5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b90e39488bbd77f2537ee13ab7864984f9d6efed60d3b540c631de406366ed67"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

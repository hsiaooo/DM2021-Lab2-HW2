{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 蕭靖澂 Ching-Cheng Hsiao\n",
    "\n",
    "Student ID: 110033632\n",
    "\n",
    "GitHub ID: hsiaooo\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the json\n",
    "path     = os.path.dirname(os.path.abspath(\"tweets_DM.json\"))\n",
    "raw_data = pd.read_json(path+\"/dataset/tweets_DM.json\",lines=True)\n",
    "tweets   = pd.json_normalize(data=raw_data['_source'])\n",
    "identify = pd.read_csv(path+\"/dataset/data_identification.csv\")\n",
    "emotion  = pd.read_csv(path+\"/dataset/emotion.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'disgust', 'anticipation', 'joy', 'anticipation', 'joy', 'sadness', 'sadness', 'trust', 'sadness', 'anticipation', 'joy', 'sadness', 'joy', 'joy', 'joy', 'joy', 'joy', 'sadness', 'disgust', 'trust', 'anticipation', 'sadness', 'anger', 'joy', 'trust', 'joy', 'fear', 'joy', 'sadness', 'joy', 'anticipation', 'trust', 'joy', 'joy', 'joy', 'disgust', 'joy', 'joy', 'joy', 'anticipation', 'sadness', 'disgust', 'joy', 'joy', 'trust', 'sadness', 'joy', 'anger', 'fear']\n",
      "['0x3140b1', '0x368b73', '0x296183', '0x2bd6e1', '0x2ee1dd', '0x34cd80', '0x33f099', '0x2ae7b7', '0x2408d4', '0x2b193b', '0x215b2e', '0x349b83', '0x270297', '0x213b5e', '0x31a0e7', '0x28a0d1', '0x2c8770', '0x317e52', '0x30188c', '0x361411', '0x1e16a0', '0x255135', '0x203527', '0x27b6b2', '0x213263', '0x255bbd', '0x33a6b7', '0x30e5c4', '0x2bffa3', '0x1dfa03', '0x369a1e', '0x37db4e', '0x1c9da9', '0x1c91b2', '0x1d5ec5', '0x38b81e', '0x25e867', '0x383ed2', '0x1e539d', '0x346f76', '0x1d7368', '0x2b732b', '0x31bb2a', '0x1f8a8f', '0x3806c4', '0x21bf40', '0x22886a', '0x2e56b9', '0x351172', '0x2f813e']\n"
     ]
    }
   ],
   "source": [
    "emotion_list = []\n",
    "id_list = []\n",
    "for i, j in zip(emotion['emotion'], emotion['tweet_id']):\n",
    "    if ',' in i:\n",
    "        i = i.split(',')[1]\n",
    "    emotion_list.append(i)\n",
    "    id_list.append(j)\n",
    "print(emotion_list[:50])\n",
    "print(id_list[:50])\n",
    "emotion_dict = {'tweet_id':id_list, 'emotion':emotion_list}\n",
    "emotion_2 = pd.DataFrame(emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet.hashtags tweet.tweet_id  \\\n",
       "0                     [Snapchat]       0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]       0x2d5350   \n",
       "2                   [bibleverse]       0x28b412   \n",
       "3                             []       0x1cd5b0   \n",
       "4                             []       0x2de201   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text identification  \n",
       "0  People who post \"add me on #Snapchat\" must be ...          train  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...          train  \n",
       "2  Confident of your obedience, I write to you, k...           test  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>          train  \n",
       "4  \"Trust is not the same as faith. A friend is s...           test  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename column names\n",
    "tweets = tweets.rename(index=str, columns={\"tweet.text\":\"text\", \"tweet.tweet_id\":\"tweet_id\", \"tweet.hashtags\":\"hashtags\"})\n",
    "\n",
    "# add identify tags to dataframe\n",
    "tweets = pd.merge(tweets, identify, on=\"tweet_id\")\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "def remove_punctuation_emoji(value):\n",
    "    value = remove_emojis(value)\n",
    "    return ''.join([c for c in value if (c not in punctuation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now ISSA is stalking Tasha  LH\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(tweets['text']):\n",
    "    tweets['text'][i] = remove_punctuation_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "   tweet_id                                               text\n",
      "0  0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
      "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
      "3  0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>\n",
      "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...\n",
      "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>\n",
      "\n",
      "Testing set\n",
      "    tweet_id                                               text\n",
      "2   0x28b412  Confident of your obedience, I write to you, k...\n",
      "4   0x2de201  \"Trust is not the same as faith. A friend is s...\n",
      "9   0x218443  When do you have enough ? When are you satisfi...\n",
      "30  0x2939d5  God woke you up, now chase the day #GodsPlan #...\n",
      "33  0x26289a  In these tough times, who do YOU turn to as yo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# get training set and test set\n",
    "train_df = tweets[tweets[\"identification\"] == \"train\"]\n",
    "test_df  = tweets[tweets[\"identification\"] == \"test\"]\n",
    "\n",
    "# drop identification tags\n",
    "train_df.drop(columns=[\"identification\"], inplace=True)\n",
    "test_df.drop(columns=[\"identification\"], inplace=True)\n",
    "\n",
    "# drop hashtags\n",
    "train_df.drop(columns=[\"hashtags\"], inplace=True)\n",
    "test_df.drop(columns=[\"hashtags\"], inplace=True)\n",
    "\n",
    "print(f'Training set\\n{train_df.head()}\\n')\n",
    "print(f'Testing set\\n{test_df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emotion column\n",
    "train_df = pd.merge(train_df, emotion_2, on=\"tweet_id\")\n",
    "test_df = test_df.assign(emotion=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2939d5</th>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x26289a</th>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text emotion\n",
       "tweet_id                                                           \n",
       "0x28b412  Confident of your obedience, I write to you, k...        \n",
       "0x2de201  \"Trust is not the same as faith. A friend is s...        \n",
       "0x218443  When do you have enough ? When are you satisfi...        \n",
       "0x2939d5  God woke you up, now chase the day #GodsPlan #...        \n",
       "0x26289a  In these tough times, who do YOU turn to as yo...        "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use tweet_id as index\n",
    "train_df.set_index(\"tweet_id\",inplace=True)\n",
    "test_df.set_index(\"tweet_id\",inplace=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d755c</th>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2c91a8</th>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text       emotion\n",
       "tweet_id                                                                 \n",
       "0x376b20  People who post \"add me on #Snapchat\" must be ...  anticipation\n",
       "0x2d5350  @brianklaas As we see, Trump is dangerous to #...       sadness\n",
       "0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>          fear\n",
       "0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy\n",
       "0x2c91a8       Still waiting on those supplies Liscus. <LH>  anticipation"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df.csv\", line_terminator=\"\\r\\n\", index=False)\n",
    "test_df.to_csv(\"test_df.csv\", line_terminator=\"\\r\\n\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to pickle file\n",
    "# train_df.to_pickle(\"train_df_emo.pkl\")\n",
    "# test_df.to_pickle(\"test_df_emo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/train_df_nohashtags.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"/test_df_nohashtags.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_df\n",
      "                                                        text       emotion\n",
      "tweet_id                                                                 \n",
      "0x376b20  People who post \"add me on #Snapchat\" must be ...  anticipation\n",
      "0x2d5350  @brianklaas As we see, Trump is dangerous to #...       sadness\n",
      "0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>          fear\n",
      "0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy\n",
      "0x2c91a8       Still waiting on those supplies Liscus. <LH>  anticipation\n",
      "...                                                     ...           ...\n",
      "0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...           joy\n",
      "0x38959e  In every circumtance I'd like to be thankful t...           joy\n",
      "0x2cbca6  there's currently two girls walking around the...           joy\n",
      "0x24faed  Ah, corporate life, where you can date <LH> us...           joy\n",
      "0x34be8c             Blessed to be living #Sundayvibes <LH>           joy\n",
      "\n",
      "[1455563 rows x 2 columns]\n",
      "\n",
      "Test_df\n",
      "                                                        text emotion\n",
      "tweet_id                                                           \n",
      "0x28b412  Confident of your obedience, I write to you, k...        \n",
      "0x2de201  \"Trust is not the same as faith. A friend is s...        \n",
      "0x218443  When do you have enough ? When are you satisfi...        \n",
      "0x2939d5  God woke you up, now chase the day #GodsPlan #...        \n",
      "0x26289a  In these tough times, who do YOU turn to as yo...        \n",
      "...                                                     ...     ...\n",
      "0x2913b4  \"For this is the message that ye heard from th...        \n",
      "0x2a980e  \"There is a lad here, which hath five barley l...        \n",
      "0x316b80  When you buy the last 2 tickets remaining for ...        \n",
      "0x29d0cb  I swear all this hard work gone pay off one da...        \n",
      "0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...        \n",
      "\n",
      "[411972 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train_df\\n\", train_df)\n",
    "print(\"\\nTest_df\\n\", test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=50000, stop_words='english',\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f0342c32d00>>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(preserve_case=False)\n",
    "# tknzr = word_tokenize()\n",
    "# tknzr = StanfordTokenizer()\n",
    "tfidf = TfidfVectorizer(max_features=50000, stop_words='english', tokenizer=tknzr.tokenize)\n",
    "\n",
    "# fitting\n",
    "tfidf.fit(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 50000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming training sets\n",
    "X_train = tfidf.transform(train_df['text'])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972, 50000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming testing sets\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pointers\n",
    "y_train = train_df['emotion']\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id\n",
      "0x28b412    \n",
      "0x2de201    \n",
      "0x218443    \n",
      "0x2939d5    \n",
      "Name: emotion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = X_train\n",
    "y = y_train\n",
    "grid_X_train, grid_X_test, grid_y_train, grid_y_test=train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "# logreg=LogisticRegression(solver='lbfgs', max_iter=100)\n",
    "# logreg_cv=GridSearchCV(logreg,grid, cv=10, scoring=\"f1_micro\")\n",
    "# logreg_cv.fit(grid_X_train, grid_y_train)\n",
    "\n",
    "# print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "# print(\"accuracy :\", logreg_cv.best_score_)\n",
    "# print('extimator: ', logreg_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After use gridsearchcv to find the parameters, the result is below.</br>\n",
    "##### Logistic regression\n",
    "tuned hpyerparameters :(best parameters)  {'C': 1000.0, 'penalty': 'l2'}</br>\n",
    "accuracy : 0.5562384343537021</br>\n",
    "extimator:  LogisticRegression(C=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg2 = LogisticRegression(C=1000, penalty=\"l2\")\n",
    "# logreg2.fit(grid_X_train,grid_y_train)\n",
    "# print(\"score: \", logreg2.score(grid_X_test, grid_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression\n",
    "Validation score:  0.555297032763947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the X_train, X_test, y_train, y_test for Logistic regression\n",
    "lr_X_train = X_train\n",
    "lr_X_test  = X_test\n",
    "lr_y_train = y_train\n",
    "lr_y_test  = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=6, max_iter=1000, n_jobs=-1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=6, n_jobs=-1, max_iter=1000)\n",
    "lr.fit(lr_X_train,lr_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result_lr = lr.predict(lr_X_test)\n",
    "pred_result_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to  /home/neaf-2070/code/hsiao/data_mining/DMLab2/DM2021-Lab2-HW2 /submission/lr_tfidf50000_nohas.csv\n"
     ]
    }
   ],
   "source": [
    "# save the result\n",
    "test_df['emotion']=pred_result_lr\n",
    "test_df.drop(columns=['text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns=['emotion']\n",
    "test_df.to_csv(path+'/submission/lr_tfidf50000_nohas.csv')\n",
    "print('Saved file to ', path, '/submission/lr_tfidf50000_nohas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I use C=1000, penalty=\"l2\", submit to kaggle, the score only 0.45819, less than use C=6, score is 0.45963."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/train_df.pkl\")\n",
    "test_df = pd.read_pickle(path+\"/test_df.pkl\")\n",
    "\n",
    "# copy the X_train, X_test, y_train, y_test for XGBoost\n",
    "xgb_X_train = X_train\n",
    "xgb_X_test  = X_test\n",
    "xgb_y_train = y_train\n",
    "xgb_y_test  = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id\n",
      "0x376b20    0\n",
      "0x2d5350    1\n",
      "0x1cd5b0    2\n",
      "0x1d755c    3\n",
      "0x2c91a8    0\n",
      "           ..\n",
      "0x321566    3\n",
      "0x38959e    3\n",
      "0x2cbca6    3\n",
      "0x24faed    3\n",
      "0x34be8c    3\n",
      "Name: emotion, Length: 1455563, dtype: object\n",
      "anger \t\t=  4\n",
      "anticipation \t=  0\n",
      "disgust \t=  6\n",
      "fear \t\t=  2\n",
      "joy \t\t=  3\n",
      "sadness \t=  1\n",
      "surprise \t=  7\n",
      "trust \t\t=  5\n"
     ]
    }
   ],
   "source": [
    "emotion = pd.Categorical(xgb_y_train.iloc[:], categories=xgb_y_train.iloc[:].unique(), ordered=True)\n",
    "\n",
    "xgb_y_train.iloc[:] = emotion.codes\n",
    "print(xgb_y_train)\n",
    "print('anger \\t\\t= ', xgb_y_train[9])\n",
    "print('anticipation \\t= ', xgb_y_train[0])\n",
    "print('disgust \\t= ', xgb_y_train[18])\n",
    "print('fear \\t\\t= ', xgb_y_train[2])\n",
    "print('joy \\t\\t= ', xgb_y_train[3])\n",
    "print('sadness \\t= ', xgb_y_train[1])\n",
    "print('surprise \\t= ', xgb_y_train[19])\n",
    "print('trust \\t\\t= ', xgb_y_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgbc = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgbc.fit(xgb_X_train, xgb_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result_xgbc = xgbc.predict(xgb_X_test)\n",
    "\n",
    "pred_result_xgbc.shape\n",
    "print(pred_result_xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_result_xgbc[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = []\n",
    "for i, emo in enumerate(pred_result_xgbc):\n",
    "    if emo == 0:\n",
    "        pred_result.append('anticipation')\n",
    "    elif emo == 1:\n",
    "        pred_result.append('sadness')\n",
    "    elif emo == 2:\n",
    "        pred_result.append('fear')\n",
    "    elif emo == 3:\n",
    "        pred_result.append('joy')\n",
    "    elif emo == 4:\n",
    "        pred_result.append('anger')\n",
    "    elif emo == 5:\n",
    "        pred_result.append('trust')\n",
    "    elif emo == 6:\n",
    "        pred_result.append('disgust')\n",
    "    elif emo == 7:\n",
    "        pred_result.append('surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pred_result)\n",
    "print(pred[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "test_df['emotion'] = pred\n",
    "test_df.drop(columns = ['hashtags','text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns = ['emotion']\n",
    "test_df.to_csv(path+'/submission/xgbc_tfidf.csv')\n",
    "print('Saved file to ', path, '/submission/xgbc_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "DT_X_train = X_train\n",
    "DT_y_train = y_train\n",
    "DT_X_test = X_test\n",
    "DT_y_test = y_test\n",
    "\n",
    "# take a look at data dimension \n",
    "print('X_train.shape: ', DT_X_train.shape)\n",
    "print('y_train.shape: ', DT_y_train.shape)\n",
    "print('X_test.shape: ', DT_X_test.shape)\n",
    "print('y_test.shape: ', DT_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = pd.Categorical(DT_y_train.iloc[:], categories=DT_y_train.iloc[:].unique(), ordered=True)\n",
    "\n",
    "DT_y_train.iloc[:] = emotion.codes\n",
    "DT_y_train = DT_y_train.astype('int')\n",
    "print(DT_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# training!\n",
    "DT_model = DT_model.fit(DT_X_train, DT_y_train)\n",
    "\n",
    "# predict!\n",
    "y_test_pred = DT_model.predict(DT_X_test)\n",
    "\n",
    "# so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "test_df['emotion']= y_test_pred\n",
    "test_df.drop(columns=['hashtags','text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns=['emotion']\n",
    "test_df.to_csv(path+'/submission/DT_tfidf.csv')\n",
    "print('Saved file to ', path, '/submission/DT_tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import rmsprop_v2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/train_df.pkl\")\n",
    "test_df = pd.read_pickle(path+\"/test_df.pkl\")\n",
    "\n",
    "# set pointers\n",
    "y_train = train_df['emotion']\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 300\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tok.texts_to_sequences(train_df['text'])\n",
    "test_seq = tok.texts_to_sequences(test_df['text'])\n",
    "\n",
    "train_seq_mat = sequence.pad_sequences(train_seq,maxlen=max_len)\n",
    "test_seq_mat = sequence.pad_sequences(test_seq,maxlen=max_len)\n",
    "\n",
    "print(train_seq_mat.shape)\n",
    "print(test_seq_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "# y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emotion = pd.Categorical(y_train.iloc[:], categories=y_train.iloc[:].unique(), ordered=True)\n",
    "# test_emotion = pd.Categorical(y_test.iloc[:], categories=y_test.iloc[:].unique(), ordered=True)\n",
    "\n",
    "# y_train.iloc[:] = train_emotion.codes\n",
    "# y_test.iloc[:] = test_emotion.codes\n",
    "\n",
    "# print(y_train)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(name='inputs',shape=[max_len])\n",
    "## Embedding\n",
    "layer = Embedding(max_words+1,128,input_length=max_len)(inputs)\n",
    "layer = LSTM(128)(layer)\n",
    "layer = Dense(128,activation=\"relu\",name=\"FC1\")(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(output_shape,activation=\"softmax\",name=\"FC2\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=rmsprop_v2.RMSprop(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(train_seq_mat, y_train,batch_size=128, epochs=3, callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the result using our model\n",
    "pred_result_lstm = label_decode(label_encoder, model.predict(test_seq_mat, batch_size=128))\n",
    "pred_result_lstm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "test_df['emotion']=pred_result_lstm\n",
    "test_df.drop(columns=['hashtags','text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns=['emotion']\n",
    "test_df.to_csv(path+'/submission/keras_tfidf.csv')\n",
    "print('Saved file to ', path, '/submission/keras_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "#MODEL_NAME = 'distilbert-base-uncased'\n",
    "MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # For tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play with BERTTokenizer\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with BERTTokenizer.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with this tokenizer a little bit before we go on.\n",
    "\n",
    "Using this tokenizer is pretty easy: just call this object, and it processes the sentences for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2061, 1011, 2170, 1000, 3819, 3944, 1000, 2001, 2061, 15640, 1010, 2004, 2092, 2004, 12532, 4648, 4726, 2149, 2013, 2746, 2000, 2115, 4418, 3004, 2153, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\"\n",
    "\n",
    "embeddings = tokenizer(example)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

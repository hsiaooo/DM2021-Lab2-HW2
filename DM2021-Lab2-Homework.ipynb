{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Ëï≠ÈùñÊæÇ Ching-Cheng Hsiao\n",
    "\n",
    "Student ID: 110033632\n",
    "\n",
    "GitHub ID: hsiaooo\n",
    "\n",
    "Kaggle name: chingchenghsiao\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the kaggle dataset\n",
    "path       = os.path.dirname(os.path.abspath(\"tweets_DM.json\"))\n",
    "raw_data = pd.read_json(path+\"/dataset/tweets_DM.json\",lines=True)\n",
    "tweets   = pd.json_normalize(data=raw_data['_source'])\n",
    "identify = pd.read_csv(path+\"/dataset/data_identification.csv\")\n",
    "emotion  = pd.read_csv(path+\"/dataset/emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet.hashtags tweet.tweet_id  \\\n",
       "0                     [Snapchat]       0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]       0x2d5350   \n",
       "2                   [bibleverse]       0x28b412   \n",
       "3                             []       0x1cd5b0   \n",
       "4                             []       0x2de201   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'disgust', 'anticipation', 'joy', 'anticipation', 'joy', 'sadness', 'sadness', 'trust', 'sadness', 'anticipation', 'joy', 'sadness', 'joy', 'joy', 'joy', 'joy', 'joy', 'sadness', 'disgust', 'trust', 'anticipation', 'sadness', 'anger', 'joy', 'trust', 'joy', 'fear', 'joy', 'sadness', 'joy', 'anticipation', 'trust', 'joy', 'joy', 'joy', 'disgust', 'joy', 'joy', 'joy', 'anticipation', 'sadness', 'disgust', 'joy', 'joy', 'trust', 'sadness', 'joy', 'anger', 'fear']\n",
      "['0x3140b1', '0x368b73', '0x296183', '0x2bd6e1', '0x2ee1dd', '0x34cd80', '0x33f099', '0x2ae7b7', '0x2408d4', '0x2b193b', '0x215b2e', '0x349b83', '0x270297', '0x213b5e', '0x31a0e7', '0x28a0d1', '0x2c8770', '0x317e52', '0x30188c', '0x361411', '0x1e16a0', '0x255135', '0x203527', '0x27b6b2', '0x213263', '0x255bbd', '0x33a6b7', '0x30e5c4', '0x2bffa3', '0x1dfa03', '0x369a1e', '0x37db4e', '0x1c9da9', '0x1c91b2', '0x1d5ec5', '0x38b81e', '0x25e867', '0x383ed2', '0x1e539d', '0x346f76', '0x1d7368', '0x2b732b', '0x31bb2a', '0x1f8a8f', '0x3806c4', '0x21bf40', '0x22886a', '0x2e56b9', '0x351172', '0x2f813e']\n"
     ]
    }
   ],
   "source": [
    "# deal with emotion, seperate the id and emotion\n",
    "emotion_list = []\n",
    "id_list = []\n",
    "for i, j in zip(emotion['emotion'], emotion['tweet_id']):\n",
    "    if ',' in i:\n",
    "        i = i.split(',')[1]\n",
    "    emotion_list.append(i)\n",
    "    id_list.append(j)\n",
    "    \n",
    "print(emotion_list[:50])\n",
    "print(id_list[:50])\n",
    "\n",
    "emotion_dict = {'tweet_id':id_list, 'emotion':emotion_list}\n",
    "emotion_2 = pd.DataFrame(emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1867535, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text identification  \n",
       "0  People who post \"add me on #Snapchat\" must be ...          train  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...          train  \n",
       "2  Confident of your obedience, I write to you, k...           test  \n",
       "3                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>          train  \n",
       "4  \"Trust is not the same as faith. A friend is s...           test  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename column names\n",
    "tweets = tweets.rename(index=str, columns={\"tweet.text\":\"text\", \"tweet.tweet_id\":\"tweet_id\", \"tweet.hashtags\":\"hashtags\"})\n",
    "\n",
    "# add identify tags to dataframe\n",
    "tweets = pd.merge(tweets, identify, on=\"tweet_id\")\n",
    "\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji and punctuation\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "def remove_punctuation_emoji(value):\n",
    "    value = remove_emojis(value)\n",
    "    return ''.join([c for c in value if (c not in punctuation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1867535/1867535 [11:45:27<00:00, 44.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove the emoji, punctuation and LH, which in a lot of text, and make all word be lower\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, text in enumerate(tqdm(tweets['text'])):\n",
    "    tweets['text'][i] = remove_punctuation_emoji(text).replace(\"LH\", \"\").replace(\"  \", \" \").lower()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now issa is stalking tasha \n"
     ]
    }
   ],
   "source": [
    "print(tweets['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "   tweet_id                                               text\n",
      "0  0x376b20  people who post add me on snapchat must be deh...\n",
      "1  0x2d5350  brianklaas as we see trump is dangerous to fre...\n",
      "3  0x1cd5b0                        now issa is stalking tasha \n",
      "5  0x1d755c  riskshow thekevinallison thx for the best time...\n",
      "6  0x2c91a8            still waiting on those supplies liscus \n",
      "\n",
      "Testing set\n",
      "    tweet_id                                               text\n",
      "2   0x28b412  confident of your obedience i write to you kno...\n",
      "4   0x2de201  trust is not the same as faith a friend is som...\n",
      "9   0x218443  when do you have enough when are you satisfied...\n",
      "30  0x2939d5  god woke you up now chase the day godsplan god...\n",
      "33  0x26289a  in these tough times who do you turn to as you...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# get training set and test set\n",
    "train_df = tweets[tweets[\"identification\"] == \"train\"]\n",
    "test_df  = tweets[tweets[\"identification\"] == \"test\"]\n",
    "\n",
    "# drop identification tags\n",
    "train_df.drop(columns=[\"identification\"], inplace=True)\n",
    "test_df.drop(columns=[\"identification\"], inplace=True)\n",
    "\n",
    "# drop hashtags\n",
    "train_df.drop(columns=[\"hashtags\"], inplace=True)\n",
    "test_df.drop(columns=[\"hashtags\"], inplace=True)\n",
    "\n",
    "print(f'Training set\\n{train_df.head()}\\n')\n",
    "print(f'Testing set\\n{test_df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emotion column\n",
    "train_df = pd.merge(train_df, emotion_2, on=\"tweet_id\")\n",
    "test_df = test_df.assign(emotion=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tweet_id as index\n",
    "train_df.set_index(\"tweet_id\",inplace=True)\n",
    "test_df.set_index(\"tweet_id\",inplace=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>confident of your obedience i write to you kno...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td>trust is not the same as faith a friend is som...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>when do you have enough when are you satisfied...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2939d5</th>\n",
       "      <td>god woke you up now chase the day godsplan god...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x26289a</th>\n",
       "      <td>in these tough times who do you turn to as you...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text emotion\n",
       "tweet_id                                                           \n",
       "0x28b412  confident of your obedience i write to you kno...        \n",
       "0x2de201  trust is not the same as faith a friend is som...        \n",
       "0x218443  when do you have enough when are you satisfied...        \n",
       "0x2939d5  god woke you up now chase the day godsplan god...        \n",
       "0x26289a  in these tough times who do you turn to as you...        "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "train_df.to_pickle(\"pkl/train_df_clean.pkl\")\n",
    "test_df.to_pickle(\"pkl/test_df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/pkl/train_df_clean.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"/pkl/test_df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_df\n",
      "                                                        text       emotion\n",
      "tweet_id                                                                 \n",
      "0x376b20  people who post add me on snapchat must be deh...  anticipation\n",
      "0x2d5350  brianklaas as we see trump is dangerous to fre...       sadness\n",
      "0x1cd5b0                        now issa is stalking tasha           fear\n",
      "0x1d755c  riskshow thekevinallison thx for the best time...           joy\n",
      "0x2c91a8            still waiting on those supplies liscus   anticipation\n",
      "...                                                     ...           ...\n",
      "0x321566  im so happy nowonder the name of this show hap...           joy\n",
      "0x38959e  in every circumtance id like to be thankful to...           joy\n",
      "0x2cbca6  theres currently two girls walking around the ...           joy\n",
      "0x24faed  ah corporate life where you can date using jus...           joy\n",
      "0x34be8c                  blessed to be living sundayvibes            joy\n",
      "\n",
      "[1455563 rows x 2 columns]\n",
      "\n",
      "Test_df\n",
      "                                                        text emotion\n",
      "tweet_id                                                           \n",
      "0x28b412  confident of your obedience i write to you kno...        \n",
      "0x2de201  trust is not the same as faith a friend is som...        \n",
      "0x218443  when do you have enough when are you satisfied...        \n",
      "0x2939d5  god woke you up now chase the day godsplan god...        \n",
      "0x26289a  in these tough times who do you turn to as you...        \n",
      "...                                                     ...     ...\n",
      "0x2913b4  for this is the message that ye heard from the...        \n",
      "0x2a980e  there is a lad here which hath five barley loa...        \n",
      "0x316b80  when you buy the last 2 tickets remaining for ...        \n",
      "0x29d0cb   i swear all this hard work gone pay off one day         \n",
      "0x2a6a4f  parcel2go no card left when i wasnt in so i ha...        \n",
      "\n",
      "[411972 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train_df\\n\", train_df)\n",
    "print(\"\\nTest_df\\n\", test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In Tf-idf, I used max_features=20000, 30000, 50000.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=50000, stop_words='english',\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f68cea63280>>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(preserve_case=False)\n",
    "# tknzr = word_tokenize()\n",
    "# tknzr = StanfordTokenizer()\n",
    "tfidf = TfidfVectorizer(max_features=50000, stop_words='english', tokenizer=tknzr.tokenize)\n",
    "\n",
    "# fitting\n",
    "tfidf.fit(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 50000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming training sets\n",
    "X_train = tfidf.transform(train_df['text'])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972, 50000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming testing sets\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pointers\n",
    "y_train = train_df['emotion']\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id\n",
      "0x28b412    \n",
      "0x2de201    \n",
      "0x218443    \n",
      "0x2939d5    \n",
      "Name: emotion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_vectorizer = CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(train_df['text'])\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1455563x904857 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17414299 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the result\n",
    "train_data_BOW_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_BOW_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 904857)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dimension\n",
    "train_data_BOW_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000', '0000000', '000000000000',\n",
       "       '00000000000000', '000000000000000001', '0000000000001'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names = BOW_vectorizer.get_feature_names_out()\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 20000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_20000 = CountVectorizer(max_features=20000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_20000.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_20000 = BOW_20000.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_20000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000000',\n",
       " '1000am',\n",
       " '1000pm',\n",
       " '1000s',\n",
       " '100daysofgratitude']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names_20000 = BOW_20000.get_feature_names()\n",
    "feature_names_20000[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the X_train, X_test, y_train, y_test for Logistic regression\n",
    "lr_X_train = X_train\n",
    "lr_X_test  = X_test\n",
    "lr_y_train = y_train\n",
    "lr_y_test  = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=6, max_iter=1000, n_jobs=-1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit the X_train and y_train\n",
    "lr = LogisticRegression(C=6, n_jobs=-1, max_iter=1000)\n",
    "lr.fit(lr_X_train,lr_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the result\n",
    "pred_result_lr = lr.predict(lr_X_test)\n",
    "pred_result_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model\n",
    "f = open('model/lr_tfidf50000_clean.pickle', 'wb')\n",
    "pickle.dump(lr, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to  /home/neaf-2070/code/hsiao/data_mining/DMLab2/DM2021-Lab2-HW2 /submission/lr_tfidf50000_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# save the result\n",
    "test_df['emotion']=pred_result_lr\n",
    "test_df.drop(columns=['text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns=['emotion']\n",
    "test_df.to_csv(path+'/submission/lr_tfidf50000_clean.csv')\n",
    "print('Saved file to ', path, '/submission/lr_tfidf50000_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the GridSearchCV to find the best parameters of Logistic Regression, and the result is C = 10000, penalty = \"l2\", but if I use C=1000, penalty=\"l2\", submit to kaggle, the score only 0.45819, less than use C=6, score is 0.45963."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 50000)\n",
      "y_train.shape:  (1455563,)\n",
      "X_test.shape:  (411972, 50000)\n",
      "y_test.shape:  (411972,)\n"
     ]
    }
   ],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/pkl/train_df_clean.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"/pkl/test_df_clean.pkl\")\n",
    "\n",
    "# copy the X_train, X_test, y_train, y_test for XGBoost\n",
    "xgb_X_train = X_train\n",
    "xgb_X_test  = X_test\n",
    "xgb_y_train = y_train\n",
    "xgb_y_test  = y_test\n",
    "\n",
    "# take a look at data dimension \n",
    "print('X_train.shape: ', xgb_X_train.shape)\n",
    "print('y_train.shape: ', xgb_y_train.shape)\n",
    "print('X_test.shape: ', xgb_X_test.shape)\n",
    "print('y_test.shape: ', xgb_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id\n",
      "0x376b20    0\n",
      "0x2d5350    1\n",
      "0x1cd5b0    2\n",
      "0x1d755c    3\n",
      "0x2c91a8    0\n",
      "           ..\n",
      "0x321566    3\n",
      "0x38959e    3\n",
      "0x2cbca6    3\n",
      "0x24faed    3\n",
      "0x34be8c    3\n",
      "Name: emotion, Length: 1455563, dtype: object\n",
      "anger \t\t=  4\n",
      "anticipation \t=  0\n",
      "disgust \t=  6\n",
      "fear \t\t=  2\n",
      "joy \t\t=  3\n",
      "sadness \t=  1\n",
      "surprise \t=  7\n",
      "trust \t\t=  5\n"
     ]
    }
   ],
   "source": [
    "# label encoding the y_train to transfer 8 emotions to 0-7\n",
    "emotion = pd.Categorical(xgb_y_train.iloc[:], categories=xgb_y_train.iloc[:].unique(), ordered=True)\n",
    "\n",
    "xgb_y_train.iloc[:] = emotion.codes\n",
    "print(xgb_y_train)\n",
    "\n",
    "# see each emotion label\n",
    "print('anger \\t\\t= ', xgb_y_train[9])\n",
    "print('anticipation \\t= ', xgb_y_train[0])\n",
    "print('disgust \\t= ', xgb_y_train[18])\n",
    "print('fear \\t\\t= ', xgb_y_train[2])\n",
    "print('joy \\t\\t= ', xgb_y_train[3])\n",
    "print('sadness \\t= ', xgb_y_train[1])\n",
    "print('surprise \\t= ', xgb_y_train[19])\n",
    "print('trust \\t\\t= ', xgb_y_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, ...)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# fit the X_train and y_train\n",
    "xgbc = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgbc.fit(xgb_X_train, xgb_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 3 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# predict the result\n",
    "pred_result_xgbc = xgbc.predict(xgb_X_test)\n",
    "\n",
    "pred_result_xgbc.shape\n",
    "print(pred_result_xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 1 3 3 0 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(pred_result_xgbc[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the lable to emotion\n",
    "pred_result = []\n",
    "for i, emo in enumerate(pred_result_xgbc):\n",
    "    if emo == 0:\n",
    "        pred_result.append('anticipation')\n",
    "    elif emo == 1:\n",
    "        pred_result.append('sadness')\n",
    "    elif emo == 2:\n",
    "        pred_result.append('fear')\n",
    "    elif emo == 3:\n",
    "        pred_result.append('joy')\n",
    "    elif emo == 4:\n",
    "        pred_result.append('anger')\n",
    "    elif emo == 5:\n",
    "        pred_result.append('trust')\n",
    "    elif emo == 6:\n",
    "        pred_result.append('disgust')\n",
    "    elif emo == 7:\n",
    "        pred_result.append('surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy' 'joy' 'joy' 'sadness' 'joy' 'joy' 'anticipation' 'joy' 'joy' 'joy']\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(pred_result)\n",
    "print(pred[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "f = open('model/xgbc_tfid_clean.pickle', 'wb')\n",
    "pickle.dump(xgbc, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to  /home/neaf-2070/code/hsiao/data_mining/DMLab2/DM2021-Lab2-HW2 /submission/xgbc_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# save the result\n",
    "test_df['emotion'] = pred\n",
    "test_df.drop(columns = ['text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns = ['emotion']\n",
    "test_df.to_csv(path+'/submission/xgbc_clean.csv')\n",
    "print('Saved file to ', path, '/submission/xgbc_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 50000)\n",
      "y_train.shape:  (1455563,)\n",
      "X_test.shape:  (411972, 50000)\n",
      "y_test.shape:  (411972,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/pkl/train_df_clean.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"/pkl/test_df_clean.pkl\")\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "DT_X_train = X_train\n",
    "DT_y_train = y_train\n",
    "DT_X_test = X_test\n",
    "DT_y_test = y_test\n",
    "\n",
    "# take a look at data dimension \n",
    "print('X_train.shape: ', DT_X_train.shape)\n",
    "print('y_train.shape: ', DT_y_train.shape)\n",
    "print('X_test.shape: ', DT_X_test.shape)\n",
    "print('y_test.shape: ', DT_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id\n",
      "0x376b20    0\n",
      "0x2d5350    1\n",
      "0x1cd5b0    2\n",
      "0x1d755c    3\n",
      "0x2c91a8    0\n",
      "           ..\n",
      "0x321566    3\n",
      "0x38959e    3\n",
      "0x2cbca6    3\n",
      "0x24faed    3\n",
      "0x34be8c    3\n",
      "Name: emotion, Length: 1455563, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# label encoding the y_train to transfer 8 emotions to 0-7\n",
    "emotion = pd.Categorical(DT_y_train.iloc[:], categories=DT_y_train.iloc[:].unique(), ordered=True)\n",
    "\n",
    "DT_y_train.iloc[:] = emotion.codes\n",
    "DT_y_train = DT_y_train.astype('int')\n",
    "print(DT_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0, verbose=2)\n",
    "\n",
    "# training!\n",
    "DT_model = DT_model.fit(DT_X_train, DT_y_train)\n",
    "\n",
    "# predict!\n",
    "y_test_pred = DT_model.predict(DT_X_test)\n",
    "\n",
    "# so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the lable to emotion\n",
    "pred_result = []\n",
    "for i, emo in enumerate(y_test_pred):\n",
    "    if emo == 0:\n",
    "        pred_result.append('anticipation')\n",
    "    elif emo == 1:\n",
    "        pred_result.append('sadness')\n",
    "    elif emo == 2:\n",
    "        pred_result.append('fear')\n",
    "    elif emo == 3:\n",
    "        pred_result.append('joy')\n",
    "    elif emo == 4:\n",
    "        pred_result.append('anger')\n",
    "    elif emo == 5:\n",
    "        pred_result.append('trust')\n",
    "    elif emo == 6:\n",
    "        pred_result.append('disgust')\n",
    "    elif emo == 7:\n",
    "        pred_result.append('surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "f = open('model/DT_tfidf_clean.pickle', 'wb')\n",
    "pickle.dump(DT_model, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "test_df['emotion']= y_test_pred\n",
    "test_df.drop(columns=['text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns=['emotion']\n",
    "test_df.to_csv(path+'/submission/DT_tfidf_cleancsv')\n",
    "print('Saved file to ', path, '/submission/DT_tfidf_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:04:33.118733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:04:33.118806: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import rmsprop_v2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(path+\"/pkl/train_df_clean.pkl\")\n",
    "test_df = pd.read_pickle(path+\"/pkl/test_df_clean.pkl\")\n",
    "\n",
    "# set pointers\n",
    "y_train = train_df['emotion']\n",
    "y_test = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max_words to tokenizer\n",
    "max_words = 20000\n",
    "max_len = 300\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "\n",
    "# fitting\n",
    "tok.fit_on_texts(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 300)\n",
      "(411972, 300)\n"
     ]
    }
   ],
   "source": [
    "# give the sequences\n",
    "train_seq = tok.texts_to_sequences(train_df['text'])\n",
    "test_seq = tok.texts_to_sequences(test_df['text'])\n",
    "\n",
    "train_seq_mat = sequence.pad_sequences(train_seq, maxlen=max_len)\n",
    "test_seq_mat = sequence.pad_sequences(test_seq, maxlen=max_len)\n",
    "\n",
    "print(train_seq_mat.shape)\n",
    "print(test_seq_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " tweet_id\n",
      "0x376b20    anticipation\n",
      "0x2d5350         sadness\n",
      "0x1cd5b0            fear\n",
      "0x1d755c             joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1455563,)\n",
      "y_test.shape:  (411972,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1455563, 8)\n",
      "y_test.shape:  (411972,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    '''\n",
    "    label encoding\n",
    "    \n",
    "    :param le: LabelEncoder() function\n",
    "    :param labels: which one want to encoder\n",
    "    '''\n",
    "    enc = le.transform(labels)\n",
    "    return np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    '''\n",
    "    label decoding\n",
    "    \n",
    "    :param le: LabelEncoder() function\n",
    "    :param labels: which one want to decoer\n",
    "    '''\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "# encoding the y_train and y_test\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "# y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  50000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:05:11.337850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-26 14:05:11.339422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:05:11.339674: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:05:11.339890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:05:11.376410: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:05:11.376658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:05:11.376874: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-12-26 14:05:11.376909: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-26 14:05:11.377829: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 128)          2560128   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " FC2 (Dense)                 (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,709,256\n",
      "Trainable params: 2,709,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(name='inputs',shape=[max_len])\n",
    "## Embedding\n",
    "layer = Embedding(max_words+1,128,input_length=max_len)(inputs)\n",
    "layer = LSTM(128)(layer)\n",
    "layer = Dense(128,activation=\"relu\",name=\"FC1\")(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(output_shape,activation=\"softmax\",name=\"FC2\")(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=rmsprop_v2.RMSprop(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:05:11.597743: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1746675600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "11372/11372 [==============================] - ETA: 0s - loss: 1.3236 - accuracy: 0.5281WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "11372/11372 [==============================] - 2199s 193ms/step - loss: 1.3236 - accuracy: 0.5281\n",
      "Epoch 2/3\n",
      "11372/11372 [==============================] - ETA: 0s - loss: 1.2398 - accuracy: 0.5615WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "11372/11372 [==============================] - 2199s 193ms/step - loss: 1.2398 - accuracy: 0.5615\n",
      "Epoch 3/3\n",
      "11372/11372 [==============================] - ETA: 0s - loss: 1.2200 - accuracy: 0.5705WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "11372/11372 [==============================] - 2205s 194ms/step - loss: 1.2200 - accuracy: 0.5705\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(train_seq_mat, y_train,batch_size=128, epochs=3, callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Train epoch 5 times:*\n",
    "Epoch 1/5</br>\n",
    "11372/11372 [==============================] - 3087s 271ms/step - loss: 1.2539 - accuracy: 0.5520</br>\n",
    "Epoch 2/5</br>\n",
    "11372/11372 [==============================] - 3091s 272ms/step - loss: 1.1417 - accuracy: 0.5952</br>\n",
    "Epoch 3/5</br>\n",
    "11372/11372 [==============================] - 3088s 272ms/step - loss: 1.1145 - accuracy: 0.6066</br>\n",
    "Epoch 4/5</br>\n",
    "11372/11372 [==============================] - 3088s 272ms/step - loss: 1.1034 - accuracy: 0.6127</br>\n",
    "Epoch 5/5</br>\n",
    "11372/11372 [==============================] - 3090s 272ms/step - loss: 1.0969 - accuracy: 0.6166</br>\n",
    "\n",
    "*Acutally in epoch 5, the accuracy is decreasing gradually, it's mean that this result is overfitting, so I finally choose epoch 3 to training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 15:55:15.723784: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 494366400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'anticipation', 'joy', 'anticipation', 'trust'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the result using our model\n",
    "pred_result_lstm = label_decode(label_encoder, model.predict(test_seq_mat, batch_size=128))\n",
    "pred_result_lstm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "from keras.models import load_model\n",
    "model.save('model_clean.h5')  # creates a HDF5 file 'model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to  /home/neaf-2070/code/hsiao/data_mining/DMLab2/DM2021-Lab2-HW2 /submission/keras_tfidf_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# save the result\n",
    "test_df['emotion']=pred_result_lstm\n",
    "test_df.drop(columns=['text'],inplace=True)\n",
    "test_df.index.rename('id',inplace=True)\n",
    "test_df.columns=['emotion']\n",
    "test_df.to_csv(path+'/submission/keras_tfidf_clean.csv')\n",
    "print('Saved file to ', path, '/submission/keras_tfidf_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
